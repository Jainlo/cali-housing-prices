{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "### Source data after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source eda file\n",
    "%run eda_lujain.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lujai\\anaconda3\\envs\\ML_DS\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17994, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featues\n",
    "X = cali.drop(\"median_house_value\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17994,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target variable\n",
    "Y = cali[\"median_house_value\"]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, Y, train_size=0.7, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a categorical feature in our dataset, we must encode it first before we start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the categorical features\n",
    "\n",
    "# different columns will be transformed separately and the features generated by each transformer will be concatenated to form a single feature space\n",
    "preprocessor = ColumnTransformer(\n",
    "  remainder=\"passthrough\",\n",
    "  transformers=[\n",
    "    (\"scale\", StandardScaler(), selector(dtype_include=\"number\")),\n",
    "    (\"one-hot\", OneHotEncoder(), selector(dtype_include=\"object\"))\n",
    "  ])\n",
    "\n",
    "# fit the features on the encoder\n",
    "encoder = preprocessor.fit(X_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "In this section we're going to test two models, XGBRegressor and Random Forest Regressor. We'll train, tune, calculate the accuracy and finally compare the two predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor\n",
    "XGBRegressor is part of the library XGBoost which stands for \"Extreme Gradient Boosting\" and it is an implementation of gradient boosting trees algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xGBR = XGBRegressor().fit(X_train,y_train)\n",
    "predicted = xGBR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47703.19572423537"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBR_score = np.sqrt(mean_squared_error(y_test, predicted))\n",
    "XGBR_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR = RandomForestRegressor().fit(X_train,y_train)\n",
    "predicted = RFR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor predicion accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_score = np.sqrt(mean_squared_error(y_test, predicted))\n",
    "RFR_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametere tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor parameters\n",
    "The most commonly configured hyperparameters are the following:\n",
    "- n_estimators: The number of trees, often increased until no further improvements are seen.\n",
    "- max_depth: The maximum depth of each tree, often values are between 1 and 10.\n",
    "- eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
    "- subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
    "- colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#XGB_model = XGBRegressor()\n",
    "\n",
    "# define model evaluation method\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\n",
    "# define loss function\n",
    "#loss = 'neg_mean_absolute_error'\n",
    "\n",
    "# evaluate the model\n",
    "#scores = cross_val_score(XGB_model, X, Y, scoring=loss, cv=cv, n_jobs=-1)\n",
    "\n",
    "# get positive scores\n",
    "#scores = abs(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best hyperparameter combination, we perform grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "XGB_model = XGBRegressor()\n",
    "\n",
    "# create grid of hyperparameter values\n",
    "hyper_grid = {\n",
    "  'n_estimator': (3, 5, 7, 9),\n",
    "  'max_depth': (4, 5, 6, 7),\n",
    "  'eta': (0.3, 0.1, 0.001)\n",
    "  }\n",
    "\n",
    "# create 10 fold CV object\n",
    "kfold = KFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "# perform random search\n",
    "random_search = RandomizedSearchCV(estimator = XGB_model,param_distributions = hyper_grid, \n",
    "                n_iter = 100, cv =kfold, n_jobs = -1)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = XGBRegressor(n_estimators =)\n",
    "best_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_xgb_model.predict(X_test)\n",
    "print({'actual': y_test, 'predicted': y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor parameters\n",
    "The most commonly configured hyperparameters are the following:\n",
    "- n_estimator: number of trees in the random forest\n",
    "- max_features: number of features in consideration at every split\n",
    "- max_depth: maximum number of levels allowed in each decision tree\n",
    "- min_samples_split: minimum sample number to split a node\n",
    "- min_sample_leaf: minimum sample number that can be stored in a leaf node\n",
    "- bootstrap: method used to sample data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "RFR_model = RandomForestRegressor()\n",
    "\n",
    "# create grid of hyperparameter values\n",
    "hyper_grid = {\n",
    " 'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [5, 20, 50, 100]\n",
    "}\n",
    "\n",
    "# create 5 fold CV object\n",
    "kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# perform random search\n",
    "random_search = RandomizedSearchCV(estimator = RFR_model,param_distributions = hyper_grid, \n",
    "                n_iter = 100, cv =kfold, n_jobs = -1)\n",
    "random_search.fit(X_train, y_train)\n",
    "#grid_search = GridSearchCV(RFR_model, hyper_grid, cv=kfold, scoring=loss)\n",
    "#results = grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# show the best estimator\n",
    "#results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RFR_model = RandomForestRegressor(n_estimators =)\n",
    "best_RFR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_RFR_model.predict(X_test)\n",
    "print({'actual': y_test, 'predicted': y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d216e108d1e253960a691f297498892bdf5023c62959a45046f60e7d498f434e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
