{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "import sklearn \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cali dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read california housing dataset\n",
    "cali = pd.read_csv(\"../data/housing.csv\")\n",
    "# show dimensions\n",
    "cali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cali.corr(),cmap='mako', annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featues\n",
    "X = cali.drop(\"median_house_value\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "Y = cali[\"median_house_value\"]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for non-informative predictors\n",
    "### Feature variance\n",
    "After testing for feature variace, we could see that there were no constant features, so none were deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove low variance features\n",
    "var_thres = VarianceThreshold(threshold=0)\n",
    "# exception for categorical features\n",
    "var_thres.fit(X.drop(\"ocean_proximity\", axis=1))\n",
    "# shows which column is non constant (True)\n",
    "var_thres.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali[\"ocean_proximity\"] = cali[\"ocean_proximity\"].replace(['NEAR OCEAN', 'NEAR BAY', 'ISLAND'], 'OTHER')\n",
    "cali2 = pd.get_dummies(data=cali,drop_first=True)\n",
    "cali2 = cali2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cali2.drop('median_house_value',axis = 1)\n",
    "target = cali2['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(target,features)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(cali, labels=True, filter=\"bottom\", sort=\"ascending\", n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali[cali['total_bedrooms'].isna()].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why its missing\n",
    "\n",
    "- from the mask it seems its missing at random , and its only 207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali = cali.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for extreme values\n",
    "### outlier detection\n",
    "Note that extreme value detection and removal only applies to numerical features. To do it we first plot a boxplot of some feature, if we notice it has outliers we run the outlier removal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cali.boxplot([\"total_rooms\", \"total_bedrooms\", \"population\", \"households\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=cali[\"median_income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['median_income']:\n",
    "    q75,q25 = np.percentile(cali.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    cali.loc[cali[x] < min,x] = np.nan\n",
    "    cali.loc[cali[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"total_rooms\"]:\n",
    "    q75,q25 = np.percentile(cali.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    cali.loc[cali[x] < min,x] = np.nan\n",
    "    cali.loc[cali[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"total_bedrooms\"]:\n",
    "    q75,q25 = np.percentile(cali.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    cali.loc[cali[x] < min,x] = np.nan\n",
    "    cali.loc[cali[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"population\"]:\n",
    "    q75,q25 = np.percentile(cali.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    cali.loc[cali[x] < min,x] = np.nan\n",
    "    cali.loc[cali[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"households\"]:\n",
    "    q75,q25 = np.percentile(cali.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    cali.loc[cali[x] < min,x] = np.nan\n",
    "    cali.loc[cali[x] > max,x] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali = cali.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cali.isnull().sum()\n",
    "cali.boxplot([\"total_rooms\", \"total_bedrooms\", \"population\", \"households\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cali, plot_kws=dict());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from this plot we can see there is right skew for these features total_rooms , total_bedrooms, population, households and median_income and we want to take log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali['log_total_rooms']= np.log(cali.total_rooms)\n",
    "cali['log_total_bedrooms']= np.log(cali.total_bedrooms)\n",
    "cali['log_population']= np.log(cali.population)\n",
    "cali['log_households']= np.log(cali.households)\n",
    "cali['log_median_income']= np.log(cali.median_income)\n",
    "cali['log_median_house_value']= np.log(cali.median_house_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali[\"ocean_proximity\"] = cali[\"ocean_proximity\"].replace(['NEAR OCEAN', 'NEAR BAY', 'ISLAND'], 'OTHER')\n",
    "cali = pd.get_dummies(data=cali,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = cali.drop(['log_median_house_value','median_house_value', 'total_rooms','total_bedrooms', 'population', 'households', 'log_median_income'],axis = 1)\n",
    "target = cali['log_median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(target,features)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
    "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[3] The condition number is large, 1.46e+03. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(cali.corr()[['log_median_house_value']].sort_values(by='log_median_house_value', ascending=False), vmin=-1, vmax=1, annot=True, cmap=sns.diverging_palette(250, 30, l=65, as_cmap=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cali.drop(['median_house_value'], axis=1)\n",
    "y = np.log(cali.median_house_value) # Applying log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X), index= X.index, columns= X.colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for imbalanced distributions\n",
    "Check for imbalanced distributions\n",
    "Check for factor variable where some levels are very common while others very rare. After using value counts on ocean proximity feature, we can see that 'NEAR OCEAN', 'NEAR BAY' and 'ISLAND' have way lower value counts, in this case we can use Lumping to group them together into one category called 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values near ocean, near bay and island with other\n",
    "#cali[\"ocean_proximity\"] = cali[\"ocean_proximity\"].loc[row_indexer,col_indexer] = value\n",
    "cali[\"ocean_proximity\"] = cali[\"ocean_proximity\"].replace(['NEAR OCEAN', 'NEAR BAY', 'ISLAND'], 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding / Dummy encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variables as numeric using dummy encoding\n",
    "data_encoded=pd.get_dummies(data=cali,drop_first=True)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Transformation\n",
    "## Check for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d216e108d1e253960a691f297498892bdf5023c62959a45046f60e7d498f434e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
